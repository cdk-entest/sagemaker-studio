{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e960490f",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - Fine-tuning text generation GPT-J 6B model on domain specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38f84c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2327e",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to [Amazon SageMaker Built-in Algorithms](https://sagemaker.readthedocs.io/en/stable/algorithms/index.html)! You can use SageMaker Built-in algorithms to solve many Machine Learning tasks through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html). You can also use these algorithms through one-click in SageMaker Studio via [JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html).\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for finetuning Foundation Models and deploying the trained model for inference. The Foundation models perform Text Generation task. It takes a text string as input and predicts next words in the sequence.\n",
    "\n",
    "* **How to run inference on [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model without finetuning.**\n",
    "* **How to fine-tune [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model on a domain specific dataset, and then run inference on the fine-tuned model. In particular, the example dataset we demonstrated is [publicly available SEC filing](https://www.sec.gov/edgar/searchedgar/companysearch) of Amazon from year 2021 to 2022. The expectation is that after fine-tuning, the model should be able to generate insightful text in financial domain.**\n",
    "* **We compare the inference result for GPT-J 6B before finetuning and after finetuning.**\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091e1f6",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select Text Generation Model GTP-J 6B](#2.-Select-Text-Generation-Model-GTP-J-6B)\n",
    "3. [Run Inference on the Pre-trained Model without finetuning](#3.-Run-Inference-on-the-Pre-trained-Model-without-finetuning)\n",
    "    * [Retrieve Artifacts & Deploy an Endpoint](#3.1.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "    * [Query endpoint and parse response](#3.2.-Query-endpoint-and-parse-response)\n",
    "    * [Clean up the endpoint](#3.3.-Clean-up-the-endpoint)\n",
    "4. [Finetune the pre-trained model on a custom dataset](#4.-Fine-tune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Retrieve Training artifacts](#4.1.-Retrieve-Training-artifacts)\n",
    "    * [Set Training parameters](#4.2.-Set-Training-parameters)\n",
    "    * [Train with Automatic Model Tuning](#4.3.-Train-with-Automatic-Model-Tuning-([HPO]))\n",
    "    * [Start Training](#4.4.-Start-Training)\n",
    "    * [Extract Training performance metrics](#4.5.-Extract-Training-performance-metrics)\n",
    "    * [Deploy & run Inference on the fine-tuned model](#4.6.-Deploy-&-run-Inference-on-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007b31a",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "Before executing the notebook, there are some initial steps required for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b943ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1051f6",
   "metadata": {},
   "source": [
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a3eb07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee983c64",
   "metadata": {},
   "source": [
    "## 2. Select Text Generation Model GTP-J 6B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960ca9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e49a7",
   "metadata": {},
   "source": [
    "## 3. Run Inference on the Pre-trained Model without finetuning\n",
    "\n",
    "Using SageMaker, we can directly perform inference on the pre-trained [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model. GPT-J 6B  is an open source 6 billion parameter model released by Eleuther AI. GPT-J 6B has been trained on a large corpus of text data ([the Pile](https://pile.eleuther.ai/) dataset) and is capable of performing various natural language processing tasks such as text generation, text classification, and text summarization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f9242",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "We retrieve the deploy_image_uri, deploy_source_uri, and base_model_uri for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecbe672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. TODO\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce1248-9fa9-4617-9dfa-d432c03ad804",
   "metadata": {},
   "source": [
    "### 3.2. Query endpoint and parse response\n",
    "The model takes a text string as input and predicts next words in the sequence. We use three of following input examples.\n",
    "\n",
    "1. `This Form 10-K report shows that`\n",
    "2. `We serve consumers through`\n",
    "3. `Our vision is`\n",
    "\n",
    "**The input examples are related to company's perforamnce in financial report. You will see the outputs from the model without finetuning are limited in providing insightful contents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f12c38b-fd58-4696-824d-be15e34f5a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63f536-be9b-4e52-a464-d206998e7071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "for quota_text in [\n",
    "    # \"This Form 10-K report shows that\",\n",
    "    # \"We serve consumers through\",\n",
    "    # \"Our vision is\",\n",
    "    \"Tensorflow vs Pytorch\"\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbad7",
   "metadata": {},
   "source": [
    "### 3.3. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f8225e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Delete the SageMaker endpoint and the attached resources\n",
    "# base_model_predictor.delete_model()\n",
    "# base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70950bf9",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the pre-trained model on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f6f8c",
   "metadata": {},
   "source": [
    "Fine-tuning refers to the process of taking a pre-trained language model and retraining it for a different but related task using specific data. This approach is also known as transfer learning, which involves transferring the knowledge learned from one task to another. Large language models (LLMs) like GPT-J 6B are trained on massive amounts of unlabeled data and can be fine-tuned on domain domain datasets, making the model perform better on that specific domain. \n",
    "\n",
    "We will use financial text from SEC filings to fine tune a LLM GPT-J 6B for financial applications. \n",
    "\n",
    "\n",
    "\n",
    "- **Input**: A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file.\n",
    "    - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "    - The number of files under train and validation (if provided) should equal to one.\n",
    "- **Output**: A trained model that can be deployed for inference.\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "---\n",
    "```\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "\n",
    "GENERAL\n",
    "\n",
    "Embracing Our Future ...\n",
    "```\n",
    "---\n",
    "SEC filings data of Amazon is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4552dd0",
   "metadata": {},
   "source": [
    "### 4.1. Retrieve Training artifacts\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the latest model. Also, we do need to specify the training_instance_type to fetch train_image_uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d9fd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "training_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfe06b",
   "metadata": {},
   "source": [
    "### 4.2. Set Training parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Text Classification model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "***\n",
    "The second set of parameters are algorithm specific training hyper-parameters. It is also used for sepcifying the model name if we want to fine-tune on the model which is not present in the dropdown list.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "036bac37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-tg-train\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad02cf3",
   "metadata": {},
   "source": [
    "***\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "651f68c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': '3', 'learning_rate': '6e-06', 'per_device_train_batch_size': '4', 'per_device_eval_batch_size': '8', 'warmup_ratio': '0.1', 'train_from_scratch': 'False', 'fp16': 'True', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '2', 'logging_steps': '10', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"epoch\"] = \"3\"\n",
    "hyperparameters[\"per_device_train_batch_size\"] = \"4\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5051d41",
   "metadata": {},
   "source": [
    "### 4.3. Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c271247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d2622",
   "metadata": {},
   "source": [
    "### 4.4. Start Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d923c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-huggingface-textgener-2023-06-03-12-38-23-897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:38:24 Starting - Starting the training job...\n",
      "2023-06-03 12:38:57 Starting - Preparing the instances for training......\n",
      "2023-06-03 12:39:52 Downloading - Downloading input data......................................................\n",
      "2023-06-03 12:48:54 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-03 12:48:55,890 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-03 12:48:55,929 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-03 12:48:55,931 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-03 12:48:56,381 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.17.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.10.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.8.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/evaluate/evaluate-0.4.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/hjson/hjson-3.1.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface_hub/huggingface_hub-0.14.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ninja/ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py_cpuinfo/py_cpuinfo-9.0.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic/pydantic-1.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.26.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 1)) (5.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 1)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.14.1->-r requirements.txt (line 6)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.14.1->-r requirements.txt (line 6)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0->-r requirements.txt (line 10)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0->-r requirements.txt (line 10)) (2022.9.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->accelerate==0.17.1->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776404 sha256=274ad45864437caa4327e4700fdba6a1902f0478cc53586ee90be43c671dd244\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/c2/70/3c/1d33771264c9f3a86ce3c43f528351e6e8b87b7f96a78799dc\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, ninja, hjson, sagemaker-jumpstart-tabular-script-utilities, pydantic, huggingface-hub, deepspeed, accelerate, transformers, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.10.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.10.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.10.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.17.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.17.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.17.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 1.18.4\u001b[0m\n",
      "\u001b[34mUninstalling datasets-1.18.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-1.18.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.17.1 datasets-2.10.1 deepspeed-0.8.3 evaluate-0.4.0 hjson-3.1.0 huggingface-hub-0.14.1 ninja-1.11.1 py-cpuinfo-9.0.0 pydantic-1.10.6 sagemaker-jumpstart-tabular-script-utilities-1.0.0 transformers-4.26.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-06-03 12:49:08,716 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-03 12:49:08,716 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-03 12:49:08,840 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epoch\": \"3\",\n",
      "        \"eval_steps\": \"20\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": \"True\",\n",
      "        \"gradient_accumulation_steps\": \"2\",\n",
      "        \"learning_rate\": \"6e-06\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_steps\": \"10\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"8\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_from_scratch\": \"False\",\n",
      "        \"warmup_ratio\": \"0.1\",\n",
      "        \"weight_decay\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"jumpstart-example-huggingface-textgener-2023-06-03-12-38-23-897\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.1.2/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epoch\":\"3\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_steps\":\"10\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"seed\":\"10\",\"train_from_scratch\":\"False\",\"warmup_ratio\":\"0.1\",\"weight_decay\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.1.2/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epoch\":\"3\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_steps\":\"10\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"seed\":\"10\",\"train_from_scratch\":\"False\",\"warmup_ratio\":\"0.1\",\"weight_decay\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"jumpstart-example-huggingface-textgener-2023-06-03-12-38-23-897\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.1.2/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epoch\",\"3\",\"--eval_steps\",\"20\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"2\",\"--learning_rate\",\"6e-06\",\"--load_best_model_at_end\",\"True\",\"--logging_steps\",\"10\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"4\",\"--seed\",\"10\",\"--train_from_scratch\",\"False\",\"--warmup_ratio\",\"0.1\",\"--weight_decay\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=True\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=6e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FROM_SCRATCH=False\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --epoch 3 --eval_steps 20 --evaluation_strategy steps --fp16 True --gradient_accumulation_steps 2 --learning_rate 6e-06 --load_best_model_at_end True --logging_steps 10 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 8 --per_device_train_batch_size 4 --seed 10 --train_from_scratch False --warmup_ratio 0.1 --weight_decay 0.2\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:49:10.292: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(current_host='algo-1', epoch=3, eval_steps=20, evaluation_strategy='steps', fp16='True', gradient_accumulation_steps=2, hosts=['algo-1'], learning_rate=6e-06, load_best_model_at_end='True', logging_steps=10, max_train_samples=-1, max_val_samples=-1, model_dir='/opt/ml/model', num_gpus=4, per_device_eval_batch_size=8, per_device_train_batch_size=4, pretrained_model='/opt/ml/input/data/model', seed=10, train=None, train_alt='/opt/ml/input/data/train', train_from_scratch='False', validation='/opt/ml/input/data/validation', warmup_ratio=0.1, weight_decay=0.2).\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /tmp. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:47,083] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:47,135] [INFO] [runner.py:550:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None run_clm.py --deepspeed ds_config.json --model_name_or_path /tmp --train_file /opt/ml/input/data/train/train.txt --do_train --output_dir /opt/ml/model --num_train_epochs 3 --gradient_accumulation_steps 2 --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --logging_steps 10 --warmup_ratio 0.1 --learning_rate 6e-06 --weight_decay 0.2 --seed 10 --validation_file /opt/ml/input/data/validation/validation.txt --evaluation_strategy steps --eval_steps 20 --load_best_model_at_end --fp16\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:135:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.10.3\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:162:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:48,263] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:50,556] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=6e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=passive,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Jun03_12-51-50_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.2,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-a949013025246d7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13662.23it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2525.93it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-a949013025246d7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 860.19it/s]\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-a949013025246d7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-a949013025246d7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 824.76it/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:658] 2023-06-03 12:51:51,754 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:658] 2023-06-03 12:51:51,754 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 810.49it/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-06-03 12:51:51,762 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-06-03 12:51:51,762 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - INFO - __main__ -   Overwrite gradient_checkpointing to be True in the model config.\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - INFO - __main__ -   Overwrite use_cache to be False in the model config.\u001b[0m\n",
      "\u001b[34m06/03/2023 12:51:51 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-a949013025246d7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 773.14it/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,770 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,771 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,771 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,771 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1800] 2023-06-03 12:51:51,771 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:51,812] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2272] 2023-06-03 12:51:51,842 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2272] 2023-06-03 12:51:51,842 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2356] 2023-06-03 12:51:51,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2356] 2023-06-03 12:51:51,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:51,843] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:51,843] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:543] 2023-06-03 12:51:51,846 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:543] 2023-06-03 12:51:51,846 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:51:51,854] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34malgo-1:407:407 [0] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:407:407 [0] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34malgo-1:408:408 [1] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:410:410 [3] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:410:410 [3] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:408:408 [1] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:409:409 [2] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:409:409 [2] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:07,620] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 6.05B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.37s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.96s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2857] 2023-06-03 12:52:28,511 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2865] 2023-06-03 12:52:28,511 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2857] 2023-06-03 12:52:28,511 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2865] 2023-06-03 12:52:28,511 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.96s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2522] 2023-06-03 12:52:28,515 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2522] 2023-06-03 12:52:28,515 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 42083.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 42069.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 42325.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 42288.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:01, 48067.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:02, 47673.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:02, 47132.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:02, 46880.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 18000/106367 [00:00<00:01, 54118.95 examples/s]#015Running tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 50221.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 50267.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 49999.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 23000/106367 [00:00<00:01, 51789.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 23000/106367 [00:00<00:01, 51418.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 23000/106367 [00:00<00:01, 51342.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 53638.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 29000/106367 [00:00<00:01, 52967.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 29000/106367 [00:00<00:01, 52953.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|██▊       | 30000/106367 [00:00<00:01, 55827.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  31%|███       | 33000/106367 [00:00<00:01, 46390.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  34%|███▍      | 36000/106367 [00:00<00:01, 46448.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  34%|███▍      | 36000/106367 [00:00<00:01, 45669.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  34%|███▍      | 36000/106367 [00:00<00:01, 45607.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▉      | 42000/106367 [00:00<00:01, 54205.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▉      | 42000/106367 [00:00<00:01, 48103.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  40%|████      | 43000/106367 [00:00<00:01, 49607.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▉      | 42000/106367 [00:00<00:01, 48059.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  45%|████▌     | 48000/106367 [00:00<00:01, 53710.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  45%|████▌     | 48000/106367 [00:00<00:01, 49359.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  45%|████▌     | 48000/106367 [00:00<00:01, 49287.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  46%|████▌     | 49000/106367 [00:00<00:01, 50277.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  51%|█████     | 54000/106367 [00:01<00:00, 52715.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 55000/106367 [00:01<00:01, 50655.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  51%|█████     | 54000/106367 [00:01<00:01, 49751.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 57000/106367 [00:01<00:00, 51023.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|█████▋    | 61000/106367 [00:01<00:00, 51808.07 examples/s]#015Running tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:00, 52271.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|█████▋    | 61000/106367 [00:01<00:01, 44956.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  59%|█████▉    | 63000/106367 [00:01<00:00, 52349.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|██████▌   | 70000/106367 [00:01<00:00, 52291.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|██████▌   | 70000/106367 [00:01<00:00, 46668.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|██████▌   | 70000/106367 [00:01<00:00, 46895.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|██████▌   | 70000/106367 [00:01<00:00, 46939.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  72%|███████▏  | 77000/106367 [00:01<00:00, 54777.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  73%|███████▎  | 78000/106367 [00:01<00:00, 50950.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 76000/106367 [00:01<00:00, 48428.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  72%|███████▏  | 77000/106367 [00:01<00:00, 49853.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  79%|███████▉  | 84000/106367 [00:01<00:00, 52461.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  77%|███████▋  | 82000/106367 [00:01<00:00, 50489.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  79%|███████▉  | 84000/106367 [00:01<00:00, 51872.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 85000/106367 [00:01<00:00, 47423.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 88000/106367 [00:01<00:00, 51085.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  86%|████████▋ | 92000/106367 [00:01<00:00, 51259.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▍ | 90000/106367 [00:01<00:00, 44135.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▍ | 90000/106367 [00:01<00:00, 44180.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  94%|█████████▍| 100000/106367 [00:01<00:00, 56042.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  88%|████████▊ | 94000/106367 [00:01<00:00, 45712.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  91%|█████████ | 97000/106367 [00:01<00:00, 46993.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%|█████████ | 96000/106367 [00:01<00:00, 46343.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  94%|█████████▍| 100000/106367 [00:02<00:00, 47190.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  97%|█████████▋| 103000/106367 [00:02<00:00, 48077.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  96%|█████████▌| 102000/106367 [00:02<00:00, 47502.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|█████████▉| 106000/106367 [00:02<00:00, 46389.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|█████████▉| 106000/106367 [00:02<00:00, 48783.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|████▎     | 8000/18760 [00:00<00:00, 64012.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 54038.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 51945.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 50869.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 15000/18760 [00:00<00:00, 61370.48 examples/s]\u001b[0m\n",
      "\u001b[34mrun_clm.py:221: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\u001b[0m\n",
      "\u001b[34m06/03/2023 12:52:31 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▍  | 14000/18760 [00:00<00:00, 40747.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▍  | 14000/18760 [00:00<00:00, 40807.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▍  | 14000/18760 [00:00<00:00, 40585.99 examples/s]\u001b[0m\n",
      "\u001b[34mrun_clm.py:221: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\u001b[0m\n",
      "\u001b[34m06/03/2023 12:52:31 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12791.70 examples/s]\u001b[0m\n",
      "\u001b[34mrun_clm.py:221: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\u001b[0m\n",
      "\u001b[34m06/03/2023 12:52:31 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mrun_clm.py:221: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\u001b[0m\n",
      "\u001b[34m06/03/2023 12:52:31 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14399.67 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:07, 13276.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:07, 13222.94 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:07, 13118.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 15054.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:06, 14715.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:06, 14633.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14582.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15354.87 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 15306.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 15162.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 15131.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15721.43 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15554.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15386.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15378.27 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15667.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15900.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15706.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15697.87 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15934.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:05, 15765.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15582.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15574.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16334.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 16095.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15896.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15881.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16489.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15935.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16272.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16273.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 16033.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15819.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15836.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16542.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 16244.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16612.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16414.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16416.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16666.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 16336.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 16124.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 16123.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16375.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16755.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16537.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16544.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16581.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16449.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16253.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16228.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16487.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16671.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16479.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16449.75 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16279.83 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16587.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16391.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16377.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16849.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16385.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16187.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16177.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16918.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16481.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16721.26 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16702.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:03, 17036.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16543.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16372.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16358.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16799.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:03, 17135.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:03, 16939.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:03, 16926.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16886.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16883.24 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16679.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16671.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16975.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16591.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16791.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16774.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16443.28 examples/s]#015Grouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16678.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16499.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16479.77 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16521.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16329.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16345.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16326.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:02<00:03, 16423.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15968.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16221.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16195.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 16040.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15372.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15866.51 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15835.77 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 16078.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15448.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15271.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15262.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 16156.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 16000.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15960.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15952.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 16074.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15865.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15857.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 16186.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 16253.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15888.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 16055.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 16048.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15966.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:03<00:02, 15779.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15758.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15754.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:03<00:02, 15878.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15769.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:03<00:02, 15668.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:03<00:02, 15646.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15836.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15849.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15659.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15626.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15932.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15994.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15757.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15726.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 16078.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15668.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15895.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15866.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:01, 16315.07 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15752.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15587.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15541.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:01, 16388.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15760.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:01, 16206.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 16165.94 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15846.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 16037.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15672.08 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15612.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 16094.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15755.26 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 15931.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 15885.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15830.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 13965.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15678.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15628.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 16310.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 16145.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 16085.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 14486.94 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 16223.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 16083.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 14926.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 16022.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 16257.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 16082.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 16025.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15168.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 16130.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15901.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:01, 15358.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15874.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 16065.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15731.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15878.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15846.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 16276.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 16087.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 16041.44 examples/s]#015Grouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:05<00:00, 15638.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:05<00:00, 16032.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:05<00:00, 15850.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:05<00:00, 15800.35 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  92%|█████████▏| 98000/106367 [00:06<00:00, 15929.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  92%|█████████▏| 98000/106367 [00:06<00:00, 16206.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16527.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16481.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  94%|█████████▍| 100000/106367 [00:06<00:00, 14881.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  94%|█████████▍| 100000/106367 [00:06<00:00, 15076.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 15097.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 15052.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 102000/106367 [00:06<00:00, 14124.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 102000/106367 [00:06<00:00, 14279.08 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14358.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14322.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  98%|█████████▊| 104000/106367 [00:06<00:00, 14539.75 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  98%|█████████▊| 104000/106367 [00:06<00:00, 14685.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14242.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14201.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|█████████▉| 106000/106367 [00:06<00:00, 14744.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|█████████▉| 106000/106367 [00:06<00:00, 14858.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14622.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14580.83 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14310.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14326.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14169.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14393.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14249.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14379.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14305.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14647.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14348.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14606.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14491.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14685.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14514.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14622.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14534.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14715.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14559.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14634.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14788.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14576.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14594.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14696.80 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14636.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14709.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14665.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14632.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14568.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14554.39 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14594.84 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14468.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14687.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14389.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14425.60 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14605.72 examples/s]\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,143] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,183] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14526.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14551.72 examples/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:565] 2023-06-03 12:52:39,279 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:565] 2023-06-03 12:52:39,279 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1972] 2023-06-03 12:52:39,279 >> Loading model from /tmp.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1972] 2023-06-03 12:52:39,279 >> Loading model from /tmp.\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:1978] 2023-06-03 12:52:39,279 >> You are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:1978] 2023-06-03 12:52:39,279 >> You are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,282] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,285] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34mYou are resuming training from a checkpoint trained with 4.25.1 of Transformers but your current version is 4.26.0. This is not recommended and could yield to errors or unwanted behaviors.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,292] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:39,305] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:40,525] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:40,531] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:40,534] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:52:40,539] [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu113/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu113/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 25.79413676261902 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 25.785176753997803 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 25.773005962371826 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 25.868322610855103 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000006, betas=(0.900000, 0.999000), weight_decay=0.200000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,623] [INFO] [logging.py:93:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,640] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,640] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,640] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu113/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,727] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,728] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 1.26 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,728] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 37.42 GB, percent = 20.0%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,730] [INFO] [stage3.py:113:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:07,730] [INFO] [stage3.py:114:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu113/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 11.619879722595215 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 11.620877742767334 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 11.617574691772461 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 11.617629051208496 seconds\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,427] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,428] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,428] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 37.4 GB, percent = 20.0%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 811008 in 114 params\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,522] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,523] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,523] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 37.4 GB, percent = 20.0%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,605] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,605] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:19,605] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 37.4 GB, percent = 20.0%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,629] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,631] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,631] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.86 GB, percent = 26.7%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,892] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,893] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:24,893] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 57.82 GB, percent = 31.0%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:25,890] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:25,890] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:25,891] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 63.67 GB, percent = 34.1%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:26,186] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:26,187] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:26,187] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 73.2 GB, percent = 39.2%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:31,922] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:31,923] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:31,923] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 131.75 GB, percent = 70.6%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:31,924] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00031566619873046875 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003104209899902344 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003662109375 seconds\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,965] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,966] [INFO] [utils.py:830:see_memory_usage] MA 0.14 GB         Max_MA 0.91 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,966] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 143.04 GB, percent = 76.6%\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,966] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,966] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,967] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7feb98d2b9d0>\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,967] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[6e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,967] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fed9d00dac0>\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,968] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 6e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.2}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 6e-06, 'warmup_num_steps': 10}\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,969] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34,970] [INFO] [config.py:1007:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 6e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.2\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 6e-06, \n",
      "            \"warmup_num_steps\": 10\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0002715587615966797 seconds\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:365] 2023-06-03 12:53:34,970 >> /tmp doesn't have deepspeed checkpoints, doing nothing\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-06-03 12:53:34,970 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-06-03 12:53:34,970 >>   Num examples = 1030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-06-03 12:53:34,970 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-06-03 12:53:34,971 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-06-03 12:53:34,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-06-03 12:53:34,971 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-06-03 12:53:34,971 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-06-03 12:53:34,972 >>   Number of trainable parameters = 0\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:365] 2023-06-03 12:53:34,970 >> /tmp doesn't have deepspeed checkpoints, doing nothing\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-06-03 12:53:34,970 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-06-03 12:53:34,970 >>   Num examples = 1030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-06-03 12:53:34,970 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-06-03 12:53:34,971 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-06-03 12:53:34,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-06-03 12:53:34,971 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-06-03 12:53:34,971 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-06-03 12:53:34,972 >>   Number of trainable parameters = 0\u001b[0m\n",
      "\u001b[34m0%|          | 0/96 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2147] 2023-06-03 12:53:34,973 >> Didn't find an RNG file for process 0, if you are resuming a training that wasn't launched in a distributed fashion, reproducibility is not guaranteed.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2147] 2023-06-03 12:53:34,973 >> Didn't find an RNG file for process 0, if you are resuming a training that wasn't launched in a distributed fashion, reproducibility is not guaranteed.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34.973: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34.973: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:34.974: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.016 algo-1:409 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.016 algo-1:410 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.016 algo-1:408 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.053: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.087 algo-1:407 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.136 algo-1:410 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.136 algo-1:408 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.136 algo-1:409 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.137 algo-1:410 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.137 algo-1:408 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.137 algo-1:409 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.137 algo-1:408 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.137 algo-1:410 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:409 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:408 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:410 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:408 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:409 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:410 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.138 algo-1:409 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.204 algo-1:407 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.205 algo-1:407 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.206 algo-1:407 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.206 algo-1:407 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-03 12:53:35.206 algo-1:407 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m1%|          | 1/96 [00:30<48:25, 30.58s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/96 [00:56<43:52, 28.00s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/96 [01:22<42:07, 27.18s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/96 [01:49<41:04, 26.79s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/96 [02:15<40:20, 26.60s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 6/96 [02:41<39:39, 26.44s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/96 [03:07<39:07, 26.38s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 8/96 [03:34<38:41, 26.38s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 9/96 [04:00<38:09, 26.32s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [04:26<37:39, 26.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6851, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [04:26<37:39, 26.27s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 11/96 [04:52<37:12, 26.26s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 12/96 [05:19<36:45, 26.25s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 13/96 [05:45<36:19, 26.26s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 14/96 [06:11<35:54, 26.28s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 15/96 [06:37<35:27, 26.26s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 16/96 [07:04<35:01, 26.27s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 17/96 [07:30<34:35, 26.27s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 18/96 [07:56<34:08, 26.26s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 19/96 [08:22<33:39, 26.23s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [08:48<33:11, 26.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4182, 'learning_rate': 6e-06, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [08:48<33:11, 26.20s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-06-03 13:02:23,918 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-06-03 13:02:23,918 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-06-03 13:02:23,918 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-06-03 13:02:23,918 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-06-03 13:02:23,918 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-06-03 13:02:23,918 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:06<00:09,  2.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.86s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:17<00:03,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.3388671875, 'eval_runtime': 25.4979, 'eval_samples_per_second': 7.765, 'eval_steps_per_second': 0.275, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [09:14<33:11, 26.20s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  3.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 21/96 [09:40<42:17, 33.83s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 22/96 [10:06<38:55, 31.56s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 23/96 [10:33<36:26, 29.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 24/96 [10:59<34:37, 28.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 25/96 [11:25<33:13, 28.07s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 26/96 [11:51<32:08, 27.55s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 27/96 [12:18<31:12, 27.14s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed581b",
   "metadata": {},
   "source": [
    "### 4.5. Extract Training performance metrics\n",
    "***\n",
    "Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce268cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d20f9",
   "metadata": {},
   "source": [
    "## 4.6. Deploy & run Inference on the fine-tuned model\n",
    "***\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an input sentence. We follow the same steps as in [3. Run inference on the pre-trained model](#3.-Run-inference-on-the-pre-trained-model). We start by retrieving the artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `tg_estimator` that we fine-tuned.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce738168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"jumpstart-example-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22eef2",
   "metadata": {},
   "source": [
    "Next, we query the finetuned model using the same set of examples above, parse the response and print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541ccf7-d26e-4088-b52a-8fd0bf93bafc",
   "metadata": {},
   "source": [
    "The outputs from fine-tune model are generated as below. We can see that after being fine-tuned, the model can generate more insightful contents related to financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6aa706-99d1-47c2-b712-c279a89eb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "for quota_text in [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name_after_finetune\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10989f-71f7-4f0e-ba5c-5dd7b375cf69",
   "metadata": {},
   "source": [
    "We compare the outputs between the model before fine-tuning and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a1d7c-4236-4dd5-aeab-912ff582d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Input example\": [\n",
    "            \"This Form 10-K report shows that\",\n",
    "            \"We serve consumers through\",\n",
    "            \"Our vision is\",\n",
    "        ],\n",
    "        \"Output before finetuning\": res_gpt_before_finetune,\n",
    "        \"Output after finetuning\": res_gpt_finetune,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d9168",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we clean up the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f98c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e286dea",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
